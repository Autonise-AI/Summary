# Combining Residual Networks with LSTMs for LipReading


## Short Terms Used

    Visemes - Visual Units that correspond to sets of visually indistinguishable phonemes
    LVCSR - Large Vocabulary Continuous Speech Reocognition
    LRW - Lip Reading in the Wild

## Summary

    Their use of 3 layer network and training regieme out performs the attention encoder-decoder network presented by DeepMind.
    They achieve state of the art results on LRW dataset which is considered to be totally in the wild and uses sentence information for improving the results of singular words.

## Abstract

    The system is a combination of spatiotemporal convoltuional, residual and bidirectional Long Short-Term Memory networks.
    Achieves an accuracy of 83 percent on the dataset Lipreading In-The-Wild benchmark
    
## Introduction

    We can have models which directly predict word or which predict visemes.
    Predicting words is associated with giving results on short bursts of speech having 1-2 words whereas visemes were thought to enable 
    recognising large sentences. But the advances in the field have allowed predicting words to work even on long sentences.
    
    This model is for words but can also be used for visemes.
    It is composed of 3 parts - 
    1) Spatiotemporal Convolution
    2) ResNet for each time step
    3) Bi-LSTM
    
    The duration of utterance is fixed 1.28 seconds. 
    * Information regarding word boundaries is not utilized during training nor during evaluation. (This might be bad)
    
## Related Work

    Given a review of the current working happening in lip reading with accuracies. Read the paper and references for more information
    
## Database

    They have used LRW dataset and made it unnecesarily compicated by not giving the model word boundaries.
    
    Ex Sound input - "... the day's other news ..." - Output Expected - other
    
    This is pretty ridiculous way to train the model. The major reason they must have chosen this approach is because maybe the dataset does not contain word-boundaries
    
## Deep Learning Modeling and PreProcessing

### Facial landmarks and data augmentation

    Preprocessing - 
    
    1 Used 66 Facial Landmarks and then regression.
    2 Resized to 112*112
    3 Common cropping using median of the 66 facial landmarks
    4 Converted to grey-scale and for augmentation applied random cropping (+-5 pixels) and horizontal flips
    
### Spatiotemporal Front-end

    3D convolution is used for capturing the short time feature movements of the lips
    Parameters in this network are ~16K
    
### Residual Network

    Used non-pretrained ResNet with 34 layer identity mapping version and fed the 3D feature maps one by one to it
    The output is 1 dimension.
    Parameters in this network are ~21K
    
### Bi-LSTM back-end and optimization criterion

    Parameters in this network are ~2400K
    Loss is applied at every LSTM output saying the output should be the entire word as they don't have any word boundaries. Really bad implementation.
    
### Implementation Details

    Clear Implementation Details given here.
    
    * They don't first train using Bi-LSTM for backend. 
    * They used Temporal Convolutional Network and pretrained the Front-end and Residual Network. 
    * Then the beginning two sub-parts are fixed and just he Bi-LSTM is trained. 
    * Then the entire system is trained end-to-end.

## Experiments

### Baseline results

    The state-of-the-art before these guys was of Lip Reading In The Wild.
    They(The other guys) achieve 76.2% Accuracy on LRW and 97% on GRID which is state-of-the-art
    
### Results using our network

    They have done ablation studies and documented the results. They achieved 83% accuracy on LRW.
    
### Discussion and error analysis

    They have given a detailed overview of how each step in the model increased the accuracy 