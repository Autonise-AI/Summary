# 1804.03619-Looking_to_Listen_at_the_Cocktail_Party

Not a structured paper.

The main idea is that

1) We detect faces(Should be visible throughout the video
2) We create features using the faces (Feature dim = 1024) and stack them together to create some sort of image
3) We use convolution with bi-linear interpolation to get an output which has the height same as the output from the time-varying STFT.
4) We use convolution on the STFT too and concatenate the facial features in time with the voice features.
5) We use B-LSTM in time and then a FC layer to predict masks for the speakers which is multiplied with the STFT and inversed to get the voice of independent speakers
6) Models have to be trained specifically depending upon the number of people

Scope of improvement - Having a model which can work with variable number of faces and the model having a very small latency